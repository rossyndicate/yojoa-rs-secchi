---
title: "xgboost algorithm for Secchi"
author: "B Steele"
date: "2023-04-03"
output: html_document
---

```{r}
library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)

match_dir = 'data/matchups/'
model_dir = 'data/models/'
```

# Purpose

This script is a rough draft applying the `xgboost` algorithm to the Yojoa remote sensing and Secchi data. We also use a myriad of climate covariates from the ERA5 climate data in this analysis.

## Load matchup data

```{r}
#list all the files in the match directory
match = list.files(match_dir)

#load them into the enivronment
sameDay = read.csv(file.path(match_dir, match[grepl('same', match)]))
oneDay = read.csv(file.path(match_dir, match[grepl('one', match)]))
twoDay = read.csv(file.path(match_dir, match[grepl('two', match)]))
threeDay = read.csv(file.path(match_dir, match[grepl('three', match)]))
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match)]))

```

Prep the data for xgboost

```{r}
prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
  
  #Add ratios then trim to needd to columns to speed up run
  df_prep %>% 
    mutate(NR= med_Nir_corr/med_Red_corr,
           BG= med_Blue_corr/med_Green_corr,
           BR= med_Blue_corr/med_Red_corr)
}

sameDay <- prepData(sameDay)
oneDay <- prepData(oneDay)
twoDay <- prepData(twoDay)
threeDay <- prepData(threeDay)
fiveDay <- prepData(fiveDay)
```

We want to predict the `secchi` value in these datasets, so let's set the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```

## Quick xgboost run on sameDay matchups

The n is pretty low in our same day matchup dataset, but let's start here, since we can constrain variability in our dataset by applying the most strict filter on time.

### Make test and training sets

For the same day matchup dataset, let's grab 20% of the data as the 'test' set and the remainder as the training set.

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
test_sameDay <- sameDay %>%
  sample_frac(.2)

## Remove holdout data
train_sameDay <- sameDay %>% filter(!rowid %in% test_sameDay$rowid) 

hist(train_sameDay$secchi) 
hist(test_sameDay$secchi)
```

### xgboost on band data only

First, let's see what we're working with if we only use band data - we'll consider this our 'starting point'. First, identify the features to train on and the target to predict (these must match your columns.

```{r}
bandonly_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr', 'NR', 'BG', 'BR')
```

Let's format the data in the way that xgboost requires the data to be formatted:

```{r}
## Format it the way xgboost likes
dtrain_fd_bo <- xgb.DMatrix(data = as.matrix(train_sameDay[,bandonly_feats]), 
                      label = train_sameDay[,target])
dtest_fd_bo <- xgb.DMatrix(data = as.matrix(test_sameDay[,bandonly_feats]), 
                     label = test_sameDay[,target])

```

Now, let's train the actual model, we'll both do 5-fold cv and compare that RMSE to our holdout RMSE, they should be roughly the same. THESE are knobs you can turn to tune the model! They're relatively conservative as is, but you can explore different values to see if they improve the model. There's great documentation if you just google 'xgboost'.

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.1, 
               gamma=0, 
               max_depth=4, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bo <- xgb.train(params = params, 
                             data = dtrain_fd_bo, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bo, 
                                              val = dtest_fd_bo), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Cool! Both the train and the validation RMSE are close, that's great news. Let's save this model and calculate error statistics.

```{r}
save(xgb_naive_fd_bo, file = file.path(model_dir, paste0('gbtree_OpticalModel_sameDay_bandsOnly_v',Sys.Date(), '.Rdata')))

preds_fd_bo <- test_sameDay %>% mutate(predicted_fd_bo = predict(xgb_naive_fd_bo, dtest_fd_bo))

evals_fd_bo <- preds_fd_bo %>%
  summarise(rmse = rmse(secchi, predicted_fd_bo),
            mae = mae(secchi, predicted_fd_bo),
            mape = mape(secchi, predicted_fd_bo),
            bias = bias(secchi, predicted_fd_bo),
            p.bias = percent_bias(secchi, predicted_fd_bo),
            smape = smape(secchi, predicted_fd_bo)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bo, aes(x = secchi, y = predicted_fd_bo)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nsame day matchups, band data only', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

## Quick xgboost run on fiveDay matchups

Let's see what happens if we loosen our time restraint and add more matchups into the mix

### Make test and training sets

For the same day matchup dataset, let's grab 20% of the data as the 'test' set and the remainder as the training set.

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
test_fiveDay <- fiveDay %>%
  sample_frac(.2) 

## Remove holdout data
train_fiveDay <- fiveDay %>% filter(!rowid %in% test_fiveDay$rowid) 

hist(train_fiveDay$secchi)
hist(test_fiveDay$secchi)
```

### xgboost on band data only

Again, let's start with bands only.

```{r}
bandonly_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr', 'NR', 'BG', 'BR')
```

And format the data in the way that xgboost requires the data to be formatted:

```{r}
## Format it the way xgboost likes
dtrain_fd_bo <- xgb.DMatrix(data = as.matrix(train_fiveDay[,bandonly_feats]), 
                      label = train_fiveDay[,target])
dtest_fd_bo <- xgb.DMatrix(data = as.matrix(test_fiveDay[,bandonly_feats]), 
                     label = test_fiveDay[,target])

```

Now, let's train the actual model, we'll both do 5-fold cv and compare that RMSE to our holdout RMSE, they should be roughly the same. THESE are knobs you can turn to tune the model! They're relatively conservative as is, but you can explore different values to see if they improve the model. There's great documentation if you just google 'xgboost'.

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.1, 
               gamma=0, 
               max_depth=4, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bo <- xgb.train(params = params, 
                             data = dtrain_fd_bo, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bo, 
                                              val = dtest_fd_bo), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Okay, the train and test are pretty far apart (which means we're overfitted), but let's look at the data anyway

```{r}
save(xgb_naive_fd_bo, file = file.path(model_dir, paste0('gbtree_OpticalModel_fiveDay_bandsOnly_v',Sys.Date(), '.Rdata')))

preds_fd_bo <- test_fiveDay %>% mutate(predicted_fd_bo = predict(xgb_naive_fd_bo, dtest_fd_bo))

evals_fd_bo <- preds_fd_bo %>%
  summarise(rmse = rmse(secchi, predicted_fd_bo),
            mae = mae(secchi, predicted_fd_bo),
            mape = mape(secchi, predicted_fd_bo),
            bias = bias(secchi, predicted_fd_bo),
            p.bias = percent_bias(secchi, predicted_fd_bo),
            smape = smape(secchi, predicted_fd_bo)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bo, aes(x = secchi, y = predicted_fd_bo)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band data only', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

Yeah, that looks like scatter shot! Good news: our RMSE on our training set decrease by half from the same day dataset. Given that we added some uncertainty by widening our window, let's try adding in some met variables to see if those help constrain the model at all and keep it from overfitting.

## Add in the met data with the five day matchups

Let's see what happens if we add in the ERA5 met data. For this example, we'll use the 5-day summaries, meaning we've summarized the met data as the mean of the previous 5 days. Since we already made the training/test datasets, let's stick with those, but name new features.

### xgboost on band data and all the 5-day met data

In our dataset, the 5-day met summaries have the suffix '\_5'

```{r}
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr', 'NR', 'BG', 'BR', 'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'min_temp_degK_5', 'tot_precip_m_5', 'mean_wind_mps_5')
```

Now we'll format the data

```{r}
## Format it the way xgboost likes
dtrain_fd_bm5 <- xgb.DMatrix(data = as.matrix(train_fiveDay[,band_met5_feats]), 
                      label = train_fiveDay[,target])
dtest_fd_bm5 <- xgb.DMatrix(data = as.matrix(test_fiveDay[,band_met5_feats]), 
                     label = test_fiveDay[,target])

```

And now train the model

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.1, 
               gamma=0, 
               max_depth=4, 
               min_child_weight=1, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5 <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Okay, the train and test are still pretty far apart (which means we're overfitted), but let's look at the data anyway

```{r}
save(xgb_naive_fd_bm5, file = file.path(model_dir, paste0('gbtree_OpticalModel_fiveDay_bandsMet5_v',Sys.Date(), '.Rdata')))

preds_fd_bm5 <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5, dtest_fd_bm5))

evals_fd_bm5 <- preds_fd_bm5 %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```

This generally looks better than before (our adjusted r-squared is higher that without the met data) AND it's better than the one-day matchups. Let's play around with the hyper parameters and see what happens...

### Try some different hyperparmeters

```{r}
#set the parameters of the boost algo
params <- list(booster = "gbtree", 
               objective = "reg:squarederror", 
               eta=0.5, 
               gamma=1, 
               max_depth=5, 
               min_child_weight=2, 
               subsample=1, 
               colsample_bytree=1)

#run the boost algo with those settings
xgb_naive_fd_bm5_play <- xgb.train(params = params, 
                             data = dtrain_fd_bm5, 
                             nrounds = 1000, 
                             watchlist = list(train = dtrain_fd_bm5, 
                                              val = dtest_fd_bm5), 
                             print_every_n = 25, 
                             early_stopping_rounds = 10, 
                             maximize = F)
```

Getting better... let's look at the data

```{r}
save(xgb_naive_fd_bm5_play, file = file.path(model_dir, paste0('gbtree_OpticalModel_fiveDay_bandsMet5_play_v',Sys.Date(), '.Rdata')))

preds_fd_bm5_play <- test_fiveDay %>% mutate(predicted_fd_bm5 = predict(xgb_naive_fd_bm5_play, dtest_fd_bm5))

evals_fd_bm5_play <- preds_fd_bm5_play %>%
  summarise(rmse = rmse(secchi, predicted_fd_bm5),
            mae = mae(secchi, predicted_fd_bm5),
            mape = mape(secchi, predicted_fd_bm5),
            bias = bias(secchi, predicted_fd_bm5),
            p.bias = percent_bias(secchi, predicted_fd_bm5),
            smape = smape(secchi, predicted_fd_bm5)) 

```

And let's visualize the predictions:

```{r}
ggplot(preds_fd_bm5_play, aes(x = secchi, y = predicted_fd_bm5)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```
