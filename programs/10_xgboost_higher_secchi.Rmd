---
title: "xgboost algorithm for Secchi"
author: "B Steele w/Edits from Matt Ross"
date: "2023-05-25"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, comment = FALSE, message = FALSE,
                      cache = FALSE)

library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)

match_dir = 'data/matchups/'
model_dir = 'data/models/'
```

# Purpose

The original purpose of this script is to apply applying the `xgboost`
algorithm to Remote Sensing Imagery of Lake Yojoa in Honduras, to
estimate Yojoa water clarity. You can read more about this lake
[here](https://www.sciencedirect.com/science/article/pii/S0048969722015479).
We have slightly adopted the code to become a teaching demo on how to
use machine learning algorithms. We also use a myriad of climate
covariates from the ERA5 climate data in this analysis.

You can read more about xgboost all over the internet, but I like the
kaggle
[demo](https://www.kaggle.com/code/rtatman/machine-learning-with-xgboost-in-r/notebook)

## Load matchup data

```{r}
#list all the files in the match directory
match = list.files(match_dir)

prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
  
  #Add ratios then trim to needd to columns to speed up run
  df_prep %>% 
    mutate(RN= med_Red_corr/med_Nir_corr,
           BG= med_Blue_corr/med_Green_corr,
           RB= med_Red_corr/med_Blue_corr,
           GB = med_Green_corr/med_Blue_corr)
}


#load the matchup files 


# Add a col
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match) & !grepl('us', match)])) %>%
  prepData(.) %>%
  distinct(date,location, .keep_all = T) %>%
  mutate(weight = secchi/max(secchi)*10)



```


We want to predict the `secchi` value in these datasets, so let's set
the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```



### Make test, train, and validation sets

60/20/20 splits

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
train <- fiveDay %>%
  sample_frac(.6) 

test <- fiveDay %>%
  filter(!rowid %in% train$rowid) %>%
  sample_frac(0.5)

val <-  fiveDay %>%
  filter(!rowid %in% test$rowid) %>%
  filter(!rowid %in% train$rowid)





```

## Add in the met data with the five day matchups

Let's see what happens if we add in the ERA5 met data. For this example,
we'll use the 5-day summaries, meaning we've summarized the met data as
the mean of the previous 5 days. Since we already made the training/test
datasets, let's stick with those, but name new features.

### xgboost on band data and all the 5-day met data

In our dataset, the 5-day met summaries have the suffix '\_5'

```{r}
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'min_temp_degK_5',
                     'tot_precip_m_5', 'mean_wind_mps_5')
```

Now we'll format the data

```{r}
## Format it the way xgboost likes
dtrain <- xgb.DMatrix(data = as.matrix(train[,band_met5_feats]), 
                      label = train[,target],
                      weight = train$weight)

dtest <- xgb.DMatrix(data = as.matrix(test[,band_met5_feats]), 
                     label = test[,target],
                     weight = test$weight)

dval <- xgb.DMatrix(data = as.matrix(val[,band_met5_feats]), 
                     label = val[,target],
                    weight = val$weight)
```


### Parameter optimization taken from Sam Sillen

```{r, eval = F}
# 3) Hypertune xgboost parameters and save as 'best_params' 

grid_train <- expand.grid(
  max_depth= c(3,6,8),
  subsample = c(.5,.8,1),
  colsample_bytree= c(.5,.8,1),
  eta = c(0.1, 0.3),
  min_child_weight= c(3,5,7)
)






hypertune_xgboost = function(train,test, grid){
  
  params <- list(booster = "gbtree", objective = 'reg:squarederror', 
                 eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight,
                 subsample=grid$subsample, 
                 colsample_bytree=grid$colsample_bytree)
  
  xgb.naive <- xgb.train(params = params, data = dtrain, nrounds = 1000, 
                         watchlist = list(train = dtrain, val = dtest), 
                         verbose = 0,
                         early_stopping_rounds = 20)
  
  
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg,
                             mod = list(xgb.naive))
  
  return(summary) 
}


## Hypertune xgboost
xgboost_hypertune <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain,dtest,current)
  })


mod_summary <- xgboost_hypertune %>% 
  arrange(val_loss) %>%
  dplyr::slice(1:20)

best_mod <- xgboost_hypertune[xgboost_hypertune$val_loss==min(xgboost_hypertune$val_loss),]

View(mod_summary)


save(mod_summary,best_mod, file = 'data/models/paramsxg_val_weight.RData')
```



### Parameter tuning


```{r}

load('data/models/paramsxg_val_weight.RData')

optimized_booster <- mod_summary$mod[1][[1]]
# Apply best mod
preds <- val %>% 
  mutate(pred_secchi = predict(optimized_booster, dval))

evals <- preds %>%
  summarise(rmse = rmse(secchi, pred_secchi),
            mae = mae(secchi, pred_secchi),
            mape = mape(secchi, pred_secchi),
            bias = bias(secchi, pred_secchi),
            p.bias = percent_bias(secchi, pred_secchi),
            smape = smape(secchi, pred_secchi),
            r2 = cor(secchi, pred_secchi)^2) 



evals
```



## Model Performance
```{r}
ggplot(preds, aes(x = secchi, y = pred_secchi)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```


### Applying model to full data


```{r}
full_stack <- read_csv('data/upstreamRS/yojoa_corr_rrs_met_v2023-04-17.csv') %>%
  mutate(secchi = 100) %>%
  prepData(.) 




stack_xgb <- xgb.DMatrix(data = as.matrix(full_stack[,band_met5_feats]))


full_stack_simp <- full_stack %>%
  mutate(secchi = predict(optimized_booster, stack_xgb)) %>%
  select(date, location, secchi, mission) 

situ_stack <- read_csv('data/in-situ/Secchi_completedataset.csv') %>%
  mutate(secchi = as.numeric(secchi),
         date = mdy(date)) %>%
  filter(!is.na(secchi)) %>%
  mutate(mission = 'Measured') %>%
  bind_rows(full_stack_simp)


library(ggthemes)
ggplot(situ_stack %>%
         filter(location == 'E'), aes(x = date, y = secchi, color = mission,
                                      shape = mission)) + 
  geom_point() + 
  scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
  theme_few() +
  theme(legend.position = c(0.8,0.8)) + 
  scale_shape_manual(values = c(19,19,19,19,1))


```


### Whole lake secchi dynamics

```{r}
lake_avg <- situ_stack %>%
  group_by(date,mission) %>%
  summarize(across(is.numeric,mean))

ggplot(lake_avg, aes(x = date, y = secchi, color = mission,shape = mission)) + 
  geom_point() + 
  scale_color_manual(values = c('grey10','grey30','grey50','grey70','blue')) + 
  theme_few() +
  theme(legend.position = c(0.8,0.8)) + 
  scale_shape_manual(values = c(19,19,19,19,1))


```

